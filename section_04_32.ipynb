{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sd = \"runwayml/stable-diffusion-v1-5\"\n",
    "output_dir = \"./content/stable_diffusion_weights/zwx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\": \"zwx\",\n",
    "        \"class_prompt\": \"photo of a person\",\n",
    "        \"instance_data_dir\": \"./content/data/zwx\",\n",
    "        \"class_data_dir\": \"./content/data/person\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "for c in concepts_list:\n",
    "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"concepts_list.json\", \"w\") as f:\n",
    "    json.dump(concepts_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 이게 필요하겠군"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 120 800 1e-06 80\n"
     ]
    }
   ],
   "source": [
    "num_imgs = 10\n",
    "num_class_images = num_imgs * 12\n",
    "max_num_steps = num_imgs * 80\n",
    "learning_rate = 1e-6 # 0.0000001\n",
    "lr_warmup_steps = int(max_num_steps / 10)\n",
    "print(num_imgs, num_class_images, max_num_steps, learning_rate, lr_warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 이열..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kk4ever/anaconda3/envs/hg/lib/python3.10/site-packages/accelerate/accelerator.py:391: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "/home/kk4ever/anaconda3/envs/hg/lib/python3.10/site-packages/diffusers/pipelines/pipeline_loading_utils.py:212: FutureWarning: You are loading the variant fp16 from runwayml/stable-diffusion-v1-5 via `revision='fp16'` even though you can load it via `variant=`fp16`. Loading model variants via `revision='fp16'` is deprecated and will be removed in diffusers v1. Please use `variant='fp16'` instead.\n",
      "  warnings.warn(\n",
      "text_encoder/model.safetensors not found\n",
      "Loading pipeline components...:  50%|██████▌      | 3/6 [00:00<00:00,  6.75it/s]/home/kk4ever/anaconda3/envs/hg/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Loading pipeline components...: 100%|█████████████| 6/6 [00:00<00:00, 11.45it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "07/03/2024 22:59:06 - INFO - __main__ - Number of class images to sample: 120.\n",
      "Generating class images:   3%|▋                  | 1/30 [00:03<01:53,  3.92s/it]"
     ]
    }
   ],
   "source": [
    "!python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$model_sd \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$output_dir \\\n",
    "  --revision=\"fp16\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=777 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=$learning_rate \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=80 \\\n",
    "  --num_class_images=$num_class_images \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=$max_num_steps \\\n",
    "  --save_interval=10000 \\\n",
    "  --save_sample_prompt=\"zwx\" \\\n",
    "  --concepts_list=\"concepts_list.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m weights_dir \u001b[38;5;241m=\u001b[39m \u001b[43mnatsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeights directory: \u001b[39m\u001b[38;5;124m'\u001b[39m, weights_dir)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "weights_dir = natsorted(glob(output_dir + os.sep + '*'))[-1]\n",
    "print('Weights directory: ', weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def grid_img(imgs, rows=1, cols=3, scale=1):\n",
    "  assert len(imgs) == rows * cols\n",
    "\n",
    "  w, h = imgs[0].size\n",
    "  w, h = int(w*scale), int(h*scale)\n",
    "\n",
    "  grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "  grid_w, grid_h = grid.size\n",
    "\n",
    "  for i, img in enumerate(imgs):\n",
    "      img = img.resize((w,h), Image.ANTIALIAS)\n",
    "      grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "  return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = output_dir\n",
    "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key = lambda x: int(x))\n",
    "\n",
    "imgs_test = []\n",
    "\n",
    "for imgs, folder in enumerate(folders):\n",
    "  folder_path = os.path.join(weights_folder, folder)\n",
    "  image_folder = os.path.join(folder_path, \"samples\")\n",
    "  images = [f for f in os.listdir(image_folder)]\n",
    "\n",
    "  for i in images:\n",
    "    img_path = os.path.join(image_folder, i)\n",
    "    r = Image.open(img_path)\n",
    "    imgs_test.append(r)\n",
    "\n",
    "grid_img(imgs_test, rows=1, cols=4, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = weights_dir + \"/model.ckpt\"\n",
    "\n",
    "half_arg = \"--half\" # fp16\n",
    "\n",
    "!python convert_diffusers_to_original_stable_diffusion.py --model_path $weights_dir  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"Converted to ckpt and saved in {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = weights_dir\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "seed = 777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"face portrait of zwx in the snow, realistic, hd, vivid, sunset\"\n",
    "negative_prompt = \"bad anatomy, ugly, deformed, desfigured, distorted face, poorly drawn hands, poorly drawn face, poorly drawn feet, blurry, low quality, low definition, lowres, out of frame, out of image, cropped, cut off, signature, watermark\"\n",
    "num_samples = 5\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 30\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "seed = random.randint(0, 2147483647)\n",
    "print(\"Seed: {}\".format(str(seed)))\n",
    "generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    imgs = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=height, width=width,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "for img in imgs:\n",
    "    display(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
